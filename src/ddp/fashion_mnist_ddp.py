import argparse
import torch
import torch.nn as nn
import numpy as np
import os
import time
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader


import torch.distributed as dist # TODO Step 0: Include DDP import statement for convenience

# Parse input arguments
parser = argparse.ArgumentParser(description='Fashion MNIST Example',
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('--batch-size', type=int, default=32,
                    help='input batch size for training')
parser.add_argument('--epochs', type=int, default=40,
                    help='number of epochs to train')
parser.add_argument('--base-lr', type=float, default=0.01,
                    help='learning rate for a single GPU')
parser.add_argument('--target-accuracy', type=float, default=.85,
                    help='Target accuracy to stop training')
parser.add_argument('--patience', type=int, default=2,
                    help='Number of epochs that meet target before stopping')

# TODO Step 1: Add the following to the argument parser:
# number of nodes (num_nodes, type = int, default = 1), 
# ID for the current node (node_id, type = int, default = 0)
# number of GPUs in each node (num_gpus, type = int, default = 1)
parser.add_argument('--num-nodes', type=int, default=1,
                    help='Number of nodes')
parser.add_argument('--node-id', type=int, default=0,
                    help='Number of ID for the current node')
parser.add_argument('--num-gpus', type=int, default=1,
                    help='Number of GPUs in each node')


args = parser.parse_args()

# TODO Step 2: Compute world size (WORLD_SIZE) using num_gpus and num_nodes
# and specify the IP address/port number for the node associated with 
# the main process (global rank = 0):
world_size = args.num_gpus * args.num_nodes
os.environ['MASTER_ADDR'] = 'localhost' 
os.environ['MASTER_PORT'] = '9956' 
os.environ['WORLD_SIZE'] = str(world_size)

# Standard convolution block followed by batch normalization 
class cbrblock(nn.Module):
    def __init__(self, input_channels, output_channels):
        super(cbrblock, self).__init__()
        self.cbr = nn.Sequential(nn.Conv2d(input_channels, output_channels, kernel_size=3, stride=(1,1),
                               padding='same', bias=False), 
                               nn.BatchNorm2d(output_channels), 
                               nn.ReLU()
        )
    def forward(self, x):
        out = self.cbr(x)
        return out

# Basic residual block
class conv_block(nn.Module):
    def __init__(self, input_channels, output_channels, scale_input):
        super(conv_block, self).__init__()
        self.scale_input = scale_input
        if self.scale_input:
            self.scale = nn.Conv2d(input_channels, output_channels, kernel_size=1, stride=(1,1),
                               padding='same')
        self.layer1 = cbrblock(input_channels, output_channels)
        self.dropout = nn.Dropout(p=0.01)
        self.layer2 = cbrblock(output_channels, output_channels)
        
    def forward(self, x):
        residual = x
        out = self.layer1(x)
        out = self.dropout(out)
        out = self.layer2(out)
        if self.scale_input:
            residual = self.scale(residual)
        out = out + residual
        
        return out
    
# Overall network
class WideResNet(nn.Module):
    def __init__(self, num_classes):
        super(WideResNet, self).__init__()
        nChannels = [1, 16, 160, 320, 640]

        self.input_block = cbrblock(nChannels[0], nChannels[1])
        
        # Module with alternating components employing input scaling
        self.block1 = conv_block(nChannels[1], nChannels[2], 1)
        self.block2 = conv_block(nChannels[2], nChannels[2], 0)
        self.pool1 = nn.MaxPool2d(2)
        self.block3 = conv_block(nChannels[2], nChannels[3], 1)
        self.block4 = conv_block(nChannels[3], nChannels[3], 0)
        self.pool2 = nn.MaxPool2d(2)
        self.block5 = conv_block(nChannels[3], nChannels[4], 1)
        self.block6 = conv_block(nChannels[4], nChannels[4], 0)
        
        # Global average pooling
        self.pool = nn.AvgPool2d(7)

        # Feature flattening followed by linear layer
        self.flat = nn.Flatten()
        self.fc = nn.Linear(nChannels[4], num_classes)

    def forward(self, x):
        out = self.input_block(x)
        out = self.block1(out)
        out = self.block2(out)
        out = self.pool1(out)
        out = self.block3(out)
        out = self.block4(out)
        out = self.pool2(out)
        out = self.block5(out)
        out = self.block6(out)
        out = self.pool(out)
        out = self.flat(out)
        out = self.fc(out)
        
        return out

def train(model, optimizer, train_loader, loss_fn, device):
    model.train()
    for images, labels in train_loader:
        # Transfering images and labels to GPU if available
        labels = labels.to(device)
        images = images.to(device)
        
        # Forward pass 
        outputs = model(images)
        loss = loss_fn(outputs, labels)
        
        # Setting all parameter gradients to zero to avoid gradient accumulation
        optimizer.zero_grad()
        
        # Backward pass
        loss.backward()
        
        # Updating model parameters
        optimizer.step()

def test(model, test_loader, loss_fn, device):
    total_labels = 0
    correct_labels = 0
    loss_total = 0
    model.eval()
    with torch.no_grad():
        for images, labels in test_loader:
            # Transfering images and labels to GPU if available
            labels = labels.to(device)
            images = images.to(device)

            # Forward pass 
            outputs = model(images)
            loss = loss_fn(outputs, labels)
            
            # Extracting predicted label, and computing validation loss and validation accuracy
            predictions = torch.max(outputs, 1)[1]
            total_labels += len(labels)
            correct_labels += (predictions == labels).sum()
            loss_total += loss
    
    v_accuracy = correct_labels / total_labels
    v_loss = loss_total / len(test_loader)
    
    return v_accuracy, v_loss

# TODO Step 3: Move all code (including comments) under __name__ == '__main__' to 
# a new 'worker' function that accepts two inputs with no return value: 
# (1) the local rank (local_rank) of the process
# (2) the parsed input arguments (args)
# The following is the signature for the worker function: worker(local_rank, args)
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

def worker(local_rank, world_size, args):
    # 1. [Step 4] í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™” (ì‹ ê³ ì‹)
    # ê° ì¼ê¾¼ì´ ë°˜ì¥(Master)ì—ê²Œ ì¶œì„ ì²´í¬ë¥¼ í•©ë‹ˆë‹¤.
    global_rank = args.node_id * args.num_gpus + local_rank
    dist.init_process_group(
        backend='nccl',            # NVIDIA GPU í†µì‹ ìš© ìµœê°• ë°±ì—”ë“œ
        init_method='env://', 
        world_size=world_size, 
        rank=global_rank
    )
    
    # global_rank = args.node_id * args.num_gpus + local_rank
    # dist.init_process_group(
    #     backend='nccl', 
    #     init_method='env://', 
    #     world_size=world_size, 
    #     rank=global_rank)
    # [Step 5] ëŒ€ì¥ë§Œ ë‹¤ìš´ë¡œë“œ ë¡œì§ (ì½”ë”© í•˜ì„¸ìš”!)
    download_flag = (local_rank == 0)
    # train_set = torchvision.datasets.FashionMNIST("./data", download=download_flag, transform=transforms.ToTensor())
    train_set = torchvision.datasets.FashionMNIST("./data", download=download_flag, transforms.Compose([transforms.ToTensor()]))
    # transforms.Compose([transforms.ToTensor()]) ì—¬ëŸ¬ê°œ ì ìš©ì„ ìœ„í•œ ë°”ê¿ˆ. 
    dist.barrier() # 0ë²ˆ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
    # test_set = torchvision.datasets.FashionMNIST("./data", download=False, train=False, transform=transforms.ToTensor())
    test_set = torchvision.datasets.FashionMNIST("./data", download=False, train=False,transforms.Compose([transforms.ToTensor()]))
  
    # Composeë¥¼ ì¨ì•¼ í•˜ëŠ” ì´ìœ  (ìŒì„± ì²˜ë¦¬ ì˜ˆì‹œ)
    # transform = transforms.Compose([
    #     SoundGain(1.2),           # 1. ì†Œë¦¬ ì¦í­
    #     AddWhiteNoise(0.01),      # 2. ë°±ìƒ‰ì†ŒìŒ ì¶”ê°€ (ê°•ê±´í•œ ëª¨ë¸ì„ ìœ„í•´)
    #     transforms.ToTensor()     # 3. í…ì„œ ë³€í™˜
    # ])

    # [Step 6] ë°ì´í„° ìª¼ê°œê¸° (Sampler ì½”ë”© í•˜ì„¸ìš”!)
    train_sampler = DistributedSampler(train_set, num_replicas=world_size, rank=global_rank)
    train_loader = DataLoader(train_set, batch_size=args.batch_size, sampler=train_sampler, drop_last=True)
    test_loader = DataLoader(test_set, batch_size=args.batch_size, drop_last=True) # í…ŒìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ
    
    
    # 2. [Step 7] í˜„ì¬ í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ìš©í•  GPU ì¥ì¹˜ ì„¤ì •
    # local_rankì— ë§ëŠ” GPUë¥¼ ë‚´ ì¥ì¹˜ë¡œ ì°œí•©ë‹ˆë‹¤.
    torch.cuda.set_device(local_rank)
    device = torch.device(f'cuda:{local_rank}')
    
    # 3. ëª¨ë¸ ì´ˆê¸°í™” ë° DDP ë˜í•‘
    # ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ê³ , ë‹¤ë¥¸ GPUì˜ ëª¨ë¸ë“¤ê³¼ ë™ê¸°í™”ë˜ë„ë¡ í¬ì¥(DDP)í•©ë‹ˆë‹¤.
    model = WideResNet(num_classes=10).to(device)
    model = DDP(model, device_ids=[local_rank])
    
    # 4. ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ë„êµ¬ ì„¤ì •
    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.base_lr)
    
    # 5. [Step 6] ë°ì´í„° ë¶„ì‚° ë¡œë” ì„¤ì •
    # (ì´ ë¶€ë¶„ì€ Step 6ì—ì„œ ë°ì´í„°ì…‹ ì •ì˜ í›„ ì™„ì„±í•˜ê² ì§€ë§Œ, êµ¬ì¡°ëŠ” ì´ë ‡ìŠµë‹ˆë‹¤)
    # train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=global_rank)
    # train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=train_sampler)
    
    print(f"ğŸš€ ì¼ê¾¼ [Rank {global_rank}]ì´ GPU {local_rank}ì—ì„œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤!")

    # ì´í›„ ì—í¬í¬ ë£¨í”„ ì‹¤í–‰...
    # for epoch in range(args.epochs):
    #     train(model, optimizer, train_loader, loss_fn, device)
    total_time = 0
    for epoch in range(args.epochs):
        t0 = time.time()
        train_sampler.set_epoch(epoch)
        # 1. í•™ìŠµ ì§„í–‰
        train(model, optimizer, train_loader, loss_fn, device)
        # [Step 8 ì‹œì‘]
        # 2. ëª¨ë“  GPUê°€ í•™ìŠµ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
        dist.barrier()
        epoch_time = time.time() - t0
        total_time += epoch_time
        # 3. ì´ˆë‹¹ ì´ë¯¸ì§€ ì²˜ë¦¬ëŸ‰ ê³„ì‚° (í…ì„œë¡œ ë³€í™˜ í•„ìˆ˜!)
        images_per_sec = torch.tensor(len(train_loader) * args.batch_size / epoch_time).to(device)        
        # 4. 0ë²ˆ GPU(Master)ì—ê²Œ ëª¨ë“  GPUì˜ ì²˜ë¦¬ëŸ‰ì„ ë”í•´ì„œ ë³´ëƒ„
        # dist.reduce(images_per_sec, dst=0, op=dist.ReduceOp.SUM)
        dist.reduce(images_per_sec, dst=0, op=dist.ReduceOp.SUM)
        # [Step 9 ì‹œì‘]
        # 1. ê° GPUì—ì„œ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
        v_accuracy, v_loss = test(model, test_loader, loss_fn, device)
        
        # 2. ê²°ê³¼ê°’ì„ í…ì„œë¡œ ë¬¶ì–´ì„œ GPUì— ì˜¬ë¦½ë‹ˆë‹¤ (í‰ê·  ê³„ì‚°ìš©)
        metrics = torch.tensor([v_accuracy, v_loss]).to(device)
        
        # 3. ì¤‘ìš”!! ëª¨ë“  GPUì˜ ì„±ì ì„ ëª¨ì•„ì„œ í‰ê· (AVG)ì„ ëƒ…ë‹ˆë‹¤.
        # all_reduceëŠ” reduceì™€ ë‹¬ë¦¬ ëª¨ë“  GPUê°€ ë˜‘ê°™ì€ 'í‰ê· ê°’'ì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤.
        dist.all_reduce(metrics, op=dist.ReduceOp.AVG)
        
        avg_acc = metrics[0].item()
        avg_loss = metrics[1].item()
        # 4. ì¶œë ¥ì€ ëŒ€ì¥(Rank 0)ë§Œ í•©ë‹ˆë‹¤. (ì•ˆ ê·¸ëŸ¬ë©´ GPU ê°œìˆ˜ë§Œí¼ ë˜‘ê°™ì€ ì¤„ì´ ì°í˜€ìš”!)
        if global_rank == 0:
            print(f"Epoch = {epoch+1:2d}: Cumulative Time = {total_time:5.3f}, "
                  f"Epoch Time = {epoch_time:5.3f}, Images/sec = {images_per_sec.item():.2f}, "
                  f"Validation Loss = {avg_loss:5.3f}, Validation Accuracy = {avg_acc:5.3f}")
            # ëª©í‘œì¹˜ ë‹¬ì„± ì‹œ ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            if avg_acc >= args.target_accuracy:
                 print(f"ğŸ¯ ëª©í‘œ ì •í™•ë„ {args.target_accuracy} ë‹¬ì„±!")
        # [Step 9 ë]
        
if __name__ == '__main__':
    world_size = args.num_gpus * args.num_nodes
    os.environ['MASTER_ADDR'] = 'localhost' 
    os.environ['MASTER_PORT'] = '9956' 
    os.environ['WORLD_SIZE'] = str(world_size)
    # [Step 10] ì—¬ê¸°ê°€ ì‹œì‘ì ì…ë‹ˆë‹¤! (ì½”ë”© í•˜ì„¸ìš”!)
    import torch.multiprocessing as mp
    # ì¼ê¾¼ë“¤ì„ GPU ê°œìˆ˜ë§Œí¼ ìƒì„±í•´ì„œ worker í•¨ìˆ˜ë¡œ ë³´ëƒ…ë‹ˆë‹¤.
    mp.spawn(worker, args=(world_size, args), nprocs=args.num_gpus)
    
    # TODO Step 4: Compute the global rank (global_rank) of the spawned process as:
    # =node_id*num_gpus + local_rank.
    # To properly initialize and synchornize each process, 
    # invoke dist.init_process_group with the approrpriate parameters:
    # backend='nccl', world_size=WORLD_SIZE, rank=global_rank
    
    # TODO Step 5: initialize a download flag (download) that is true 
    # only if local_rank == 0. This download flag can be used as an 
    # input argument in torchvision.datasets.FashionMNIST.
    # Download the training and validation sets for only local_rank == 0. 
    # Call dist.barrier() to have all processes in a given node wait 
    # till data download is complete. Following this, for all other 
    # processes, torchvision.datasets.FashionMNIST can be called with
    # the download flag as false.
    
    # train_set = torchvision.datasets.FashionMNIST("./data", download=True, transform=
    #                                            transforms.Compose([transforms.ToTensor()]))
    # test_set = torchvision.datasets.FashionMNIST("./data", download=True, train=False, transform=
    #                                           transforms.Compose([transforms.ToTensor()]))  

    # TODO Step 6: generate two samplers (one for the training 
    # dataset (train_sampler) and the other for the testing 
    # dataset (test_sampler) with  torch.utils.data.distributed.DistributedSampler. 
    # Inputs to this function include:
    # (1) the datasets (either train_loader_subset or test_loader_subset)
    # (2) number of replicas (num_replicas), which is the world size (WORLD_SIZE) 
    # (3) the global rank (global_rank). 
    # Pass the appropriate sampler as a parameter (e.g., sampler = train_sampler)
    # to the training and testing DataLoader

    # Training data loader
    # train_loader = torch.utils.data.DataLoader(train_set, 
    #                                            batch_size=args.batch_size, drop_last=True)
    # Validation data loader
    # test_loader = torch.utils.data.DataLoader(test_set,
    #                                           batch_size=args.batch_size, drop_last=True)

    # Create the model and move to GPU device if available
    # num_classes = 10

    # TODO Step 7: Modify the torch.device call from "cuda:0" to "cuda:<enter local rank here>" 
    # to pin the process to its assigned GPU. 
    # After the model is moved to the assigned GPU, wrap the model with 
    # nn.parallel.DistributedDataParallel, which requires the local rank (local_rank)
    # to be specificed as the 'device_ids' parameter: device_ids=[local_rank]
    
    # device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # TODO Optional: before moving the model to the GPU, convert standard 
    # batchnorm layers to SyncBatchNorm layers using 
    # torch.nn.SyncBatchNorm.convert_sync_batchnorm. 
    
    # model = WideResNet(num_classes).to(device)

    # Define loss function
    # loss_fn = nn.CrossEntropyLoss()

    # Define the SGD optimizer
    # optimizer = torch.optim.SGD(model.parameters(), lr=args.base_lr)

    # val_accuracy = []

    # total_time = 0

    # for epoch in range(args.epochs):
        
        # t0 = time.time()
        
        # TODO Step 6.5: update the random seed of the DistributedSampler to change
        # the shuffle ordering for each epoch. It is necessary to do this for
        # the train_sampler, but irrelevant for the test_sampler. The random seed
        # can be altered with the set_epoch method (which accepts the epoch number
        # as an input) of the DistributedSampler. 
        
        # train(model, optimizer, train_loader, loss_fn, device)
        
        # TODO Step 8: At the end of every training epoch, synchronize (using dist.barrier())
        # all processes to compute the slowest epoch time. 
        # To compute the number of images processed per second, convert images_per_sec
        # into a tensor on the GPU, and then call torch.distributed.reduce on images_per_sec 
        # with global rank 0 as the destination process. The reduce operation computes the 
        # sum of images_per_sec across all GPUs and stores the sum in images_per_sec in the 
        # master process (global rank 0).
        # Once this computation is done, enable the metrics print statement for only the master process.


        
        # v_accuracy, v_loss = test(model, test_loader, loss_fn, device)
        
        # TODO Step 9: average validation accuracy and loss across all GPUs  
        # using torch.distributed.all_reduce. To perform an average operation, 
        # provide 'dist.ReduceOp.AVG' as the input for the op parameter in 
        # torch.distributed.all_reduce.  
        # dist.reduce(images_per_sec, dst=0, op=dist.ReduceOp.SUM)
        # val_accuracy.append(v_accuracy)
        
        # print("Epoch = {:2d}: Cumulative Time = {:5.3f}, Epoch Time = {:5.3f}, Images/sec = {}, Validation Loss = {:5.3f}, Validation Accuracy = {:5.3f}".format(epoch+1, total_time, epoch_time, images_per_sec, v_loss, val_accuracy[-1]))

        # if len(val_accuracy) >= args.patience and all(acc >= args.target_accuracy for acc in val_accuracy[-args.patience:]):
        #     print('Early stopping after epoch {}'.format(epoch + 1))
        #     break
            
    # TODO Step 10: Within __name__ == '__main__', launch each process (total number of 
    # processes is equivalent to the number of available GPUs per node) with 
    # torch.multiprocessing.spawn(). Input parameters include the worker function, 
    # the number of GPUs per node (nprocs), and all the parsed arguments.
